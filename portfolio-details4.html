<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Project Details</title>
  <meta content="" name="descriptison">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/icofont/icofont.min.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/venobox/venobox.css" rel="stylesheet">
  <link href="assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

</head>

<body>

  <!-- ======= Mobile nav toggle button ======= -->
  <button type="button" class="mobile-nav-toggle d-xl-none"><i class="icofont-navigation-menu"></i></button>

  <!-- ======= Header ======= -->
  <header id="header">
    <div class="d-flex flex-column">

      <div class="profile">
        <img src="assets/img/profile_1.jpg" alt="" class="img-fluid rounded-circle">
        <h1 class="text-light"><a href="index.html">Veer Kalburgi</a></h1>
        <div class="social-links mt-3 text-center">
          <a href="https://github.com/veerkalburgi" class="github"><i class="bx bxl-github"></i></a>
          <a href="https://www.linkedin.com/in/veer-kalburgi-a2100550/" class="linkedin"><i class="bx bxl-linkedin"></i></a> 
          <a href="https://scholar.google.com/citations?user=BHXBrCUAAAAJ&hl=en" class="google scholar"><i class="bx bxl-google scholar"></i></a>
        </div>
      </div>

     <nav class="nav-menu">
        <ul>
          <li class="active"><a href="index.html"><i class="bx bx-home"></i> <span>Home</span></a></li>

  
        </ul>
      </nav><!-- .nav-menu -->
      <button type="button" class="mobile-nav-toggle d-xl-none"><i class="icofont-navigation-menu"></i></button>

    </div>
  </header><!-- End Header -->

  <main id="main">

    <!-- ======= Breadcrumbs ======= -->
    <section id="breadcrumbs" class="breadcrumbs">
      <div class="container">

        <div class="d-flex justify-content-between align-items-center">
          <h2>Portfoio Details</h2>
          <ol>
            <li><a href="index.html">Home</a></li>
            <li>Portfoio Details</li>
          </ol>
        </div>

      </div>
    </section><!-- End Breadcrumbs -->

    <!-- ======= Portfolio Details Section ======= -->
    <section id="portfolio-details" class="portfolio-details">
      <div class="container">

        <div class="portfolio-details-container">

          <div class="owl-carousel portfolio-details-carousel">
            <img src="assets/img/portfolio/portfolio_4.jpg" class="img-fluid" alt="">
          </div>

          <div class="portfolio-info">
            <h3>Reinforcement-Learning-Specialization</h3>
            <ul>
              <li><strong>Category</strong>: RL</li>
              <li><strong>Project date</strong>: September, 2020</li>
              <li><strong>Project URL</strong>: <a href="https://github.com/veerkalburgi/Reinforcement-Learning-Specialization">github</a></li>
            </ul>
          </div>

        </div>

        <div class="portfolio-description">
          <h2>About</h2>
          <p>
            The Reinforcement Learning Specialization consists of 4 courses exploring the power of adaptive learning systems and artificial intelligence (AI).
Harnessing the full potential of artificial intelligence requires adaptive learning systems. Learn how Reinforcement Learning (RL) solutions help solve real-world problems through trial-and-error interaction by implementing a complete RL solution from beginning to end. Learners will understand the foundations of much of modern probabilistic artificial intelligence (AI) and be prepared to take more advanced courses or to apply AI tools and ideas to real-world problems. This content will focus on “small-scale” problems in order to understand the foundations of Reinforcement Learning, as taught by world-renowned experts at the University of Alberta, Faculty of Science.
          </p>
          <li><strong>Tech Stack:</strong> Jupyter Lab, Anaconda, Python 3+, sklearn, pandas, numpy, seaborn, plotly</li>
          <br>
          <h3><strong>Fundamentals of Reinforcement Learning</strong></h3>
            <p>
            This course introduces you to statistical learning techniques where an agent explicitly takes action and interacts with the world. Understanding the importance and challenges of learning agents that make decisions is of vital importance today, with more and more companies interested in interactive agents and intelligent decision-making.
This course introduces you to the fundamentals of Reinforcement Learning. When you finish this course, you will: - Formalize problems as Markov Decision Processes - Understand basic exploration methods and the exploration/exploitation tradeoff - Understand value functions, as a general-purpose tool for optimal decision-making - Know how to implement dynamic programming as an efficient solution approach to an industrial control problem This course teaches you the key concepts of Reinforcement Learning, underlying classic and modern algorithms in RL. </p> 
            <br>
          <h3><strong>Sample-based Learning Methods</strong></h3>
            <p>
            In this course, you will learn about several algorithms that can learn near-optimal policies based on trial and error interaction with the environment---learning from the agent’s own experience. Learning from actual experience is striking because it requires no prior knowledge of the environment’s dynamics, yet can still attain optimal behavior. We will cover intuitively simple but powerful Monte Carlo methods, and temporal difference learning methods including Q-learning. We will wrap up this course investigating how we can get the best of both worlds: algorithms that can combine model-based planning (similar to dynamic programming) and temporal difference updates to radically accelerate learning.
Understand Temporal-Difference learning and Monte Carlo as two strategies for estimating value functions from sampled experience - Understand the importance of exploration, when using sampled experience rather than dynamic programming sweeps within a model - Understand the connections between Monte Carlo and Dynamic Programming and TD. - Implement and apply the TD algorithm, for estimating value functions - Implement and apply Expected Sarsa and Q-learning (two TD methods for control) - Understand the difference between on-policy and off-policy control - Understand planning with simulated experience (as opposed to classic planning strategies) - Implement a model-based approach to RL, called Dyna, which uses simulated experience - Conduct an empirical study to see the improvements in sample efficiency when using Dyna</p>            
          <br>
          <h3><strong>Prediction and Control with Function Approximation</strong></h3>
            <p>
            In this course, you will learn how to solve problems with large, high-dimensional, and potentially infinite state spaces. You will see that estimating value functions can be cast as a supervised learning problem---function approximation---allowing you to build agents that carefully balance generalization and discrimination in order to maximize reward. We will begin this journey by investigating how our policy evaluation or prediction methods like Monte Carlo and TD can be extended to the function approximation setting. You will learn about feature construction techniques for RL, and representation learning via neural networks and backprop. We conclude this course with a deep-dive into policy gradient methods; a way to learn policies directly without learning a value function. In this course, you will solve two continuous-state control tasks and investigate the benefits of policy gradient methods in a continuous-action environment.
Understand how to use supervised learning approaches to approximate value functions -Understand objectives for prediction (value estimation) under function approximation -Implement TD with function approximation (state aggregation), on an environment with an infinite state space (continuous state space) -Understand fixed basis and neural network approaches to feature construction -Implement TD with neural network function approximation in a continuous state environment -Understand new difficulties in exploration when moving to function approximation -Contrast discounted problem formulations for control versus an average reward problem formulation -Implement expected Sarsa and Q-learning with function approximation on a continuous state control task -Understand objectives for directly estimating policies (policy gradient objectives) -Implement a policy gradient method (called Actor-Critic) on a discrete state environment</p> 
            <br>
          <h3><strong>A Complete Reinforcement Learning System (Capstone)</strong></h3>
            <p>
            In this final course, you will put together your knowledge from Courses 1, 2 and 3 to implement a complete RL solution to a problem. This capstone will let you see how each component---problem formulation, algorithm selection, parameter selection, and representation design---fits together into a complete solution, and how to make appropriate choices when deploying RL in the real world. This project will require you to implement both the environment to stimulate your problem, and a control agent with Neural Network function approximation. In addition, you will conduct a scientific study of your learning system to develop your ability to assess the robustness of RL agents. To use RL in the real world, it is critical to (a) appropriately formalize the problem as an MDP, (b) select appropriate algorithms, (c ) identify what choices in your implementation will have large impacts on performance, and (d) validate the expected behavior of your algorithms. This capstone is valuable for anyone who is planning on using RL to solve real problems.
</p> 
            <br>
          
			<h1><strong>References</strong></h1>
			<p>
				<li><a href="https://www.coursera.org/learn/fundamentals-of-reinforcement-learning/home/welcome">Coursera Fundamentals of Reinforcement Learning</a> and my <a href="https://github.com/veerkalburgi/Reinforcement-Learning-Specialization/tree/master/Fundamentals%20of%20Reinforcement%20Learning">Project</a></li>
				<li><a href="https://www.coursera.org/learn/sample-based-learning-methods/home/welcome">Coursera Sample-based Learning Methods</a> and my <a href="https://github.com/veerkalburgi/Reinforcement-Learning-Specialization/tree/master/Sample-based%20Learning%20Methods">Project</a></li>
				<li><a href="https://www.coursera.org/learn/prediction-control-function-approximation/home/welcome">Coursera Prediction and Control with Function Approximation</a> and my <a href="https://github.com/veerkalburgi/Reinforcement-Learning-Specialization/tree/master/Prediction%20and%20Control%20with%20Function%20Approximation">Project</a></li>
				<li><a href="https://www.coursera.org/learn/complete-reinforcement-learning-system/home/welcome">Coursera A Complete Reinforcement Learning System (Capstone)</a> and my <a href="https://github.com/veerkalburgi/Reinforcement-Learning-Specialization/tree/master/A%20Complete%20Reinforcement%20Learning%20System%20(Capstone)">Project</a></li>
				
			</p>
        </div>

      </div>
    </section><!-- End Portfolio Details Section -->

  <!-- Site footer -->
    <footer class="site-footer">
      <div class="container">
           <div>
            <p class="text-center">Made with Love ❤️ by <a href="https://github.com/pr2tik1">Pratik</a> | Inspired by <a href="https://bootstrapmade.com/iportfolio-bootstrap-portfolio-websites-template/">Bootstrapmade.com</a></p>
          </div>
      </div>
</footer>

  </main><!-- End #main -->


  <a href="#" class="back-to-top"><i class="icofont-simple-up"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/waypoints/jquery.waypoints.min.js"></script>
  <script src="assets/vendor/counterup/counterup.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/venobox/venobox.min.js"></script>
  <script src="assets/vendor/owl.carousel/owl.carousel.min.js"></script>
  <script src="assets/vendor/typed.js/typed.min.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
